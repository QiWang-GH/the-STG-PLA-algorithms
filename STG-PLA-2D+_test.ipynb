{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data\n",
    "\n",
    "Note: data is the three scale channel features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# load_path='F:/wangqi/nistdataset/AAPlantD1_2GHz_TX1_hpol_run4_pp.mat'  # 3600:39600 120\n",
    "# load_mat=h5py.File(load_path,'r')\n",
    "# IQdata=load_mat['IQdata'][3600:39600,0:256].view('complex')\n",
    "# locations=120\n",
    "# records=300\n",
    "\n",
    "load_path='F:/wangqi/nistdataset/AAPlantD1_2GHz_TX1_vpol_run3_pp.mat'   # 6600:42600  120\n",
    "load_mat=h5py.File(load_path,'r')\n",
    "IQdata=load_mat['IQdata'][6600:42600,0:256].view('complex')\n",
    "locations=120\n",
    "records=300\n",
    "\n",
    "# load_path='F:/wangqi/nistdataset/AAPlantD3_2GHz_TX2b_vpol_run32_pp.mat'  # 3900:39900 120\n",
    "# load_mat=h5py.File(load_path,'r')\n",
    "# IQdata=load_mat['IQdata'][3900:39900,0:256].view('complex')\n",
    "# locations=120\n",
    "# records=300\n",
    "\n",
    "# load_path='F:/wangqi/nistdataset/AAPlantD3_2GHz_TX2b_hpol_run34_pp.mat'  # 3900:39900 120\n",
    "# load_mat=h5py.File(load_path,'r')\n",
    "# IQdata=load_mat['IQdata'][2100:33900,0:256].view('complex')\n",
    "# locations=106\n",
    "# records=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "seed = 0                        # random seed\n",
    "batch_size = 32                # batch size\n",
    "num_epoch = 10                  # the number of training epoch\n",
    "patience = 10                  # If no improvement in 'patience' epochs, early stop\n",
    "learning_rate = 0.0001         # learning rate\n",
    "model_path = './model.ckpt'     # the path where the checkpoint will be saved\n",
    "\n",
    "# model parameters\n",
    "k=1  #H为k+1个h\n",
    "nf=2 #参考CSI的数量+1\n",
    "index=[0,1,2]\n",
    "# index=[0,1,3,2]\n",
    "# index=[0,1,3,2,4]\n",
    "# index=[0,1,4,2,5,3,6,7]\n",
    "# index=[0,1,4,2,5,3,6]\n",
    "d=2  #eve在alice后面d-1个位置处尾随\n",
    "input_channels=4\n",
    "len_sequence = 256                  # the input dim of the model, you should not change the value\n",
    "hidden_layers = 1               # the number of hidden layers\n",
    "hidden_dim = 64                # the hidden dim\n",
    "output_dim=2\n",
    "mu=0.4\n",
    "std=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creat a image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import fft\n",
    "def convert_pic(h):\n",
    "    width, height=h.shape  # width=128,height=k+nf\n",
    "    img=np.zeros([input_channels,len_sequence,height-1]).astype(np.float32)\n",
    "    for i in range(1,height):\n",
    "        img[0,:,i-1]=np.abs(h[:,i]*np.conj(h[:,0]))\n",
    "        img[1,:,i-1]=np.angle(h[:,i]*np.conj(h[:,0]))\n",
    "    hf=fft.fft2(h)\n",
    "    for i in range(1,height):\n",
    "        img[2,:,i-1]=np.abs(hf[:,i]*np.conj(hf[:,0]))\n",
    "        img[3,:,i-1]=np.angle(hf[:,i]*np.conj(hf[:,0]))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creatH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatHimag_Alice(idx, k, nf):\n",
    "    H=np.zeros((k+nf,len_sequence)).astype(complex)\n",
    "    H=IQdata[idx-nf:idx+k]\n",
    "    # print(np.shape(H))\n",
    "    return  np.transpose(H[index])\n",
    "def creatHimag_Eve(idx, k, nf, d):\n",
    "    H=np.zeros((k+nf,len_sequence)).astype(complex)\n",
    "    H[nf:k+nf,:]=IQdata[idx-d*records:idx+k-d*records]\n",
    "    H[0:nf,:]=IQdata[idx-nf:idx]\n",
    "    return  np.transpose(H[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs=records*d\n",
    "num_dataall=locations*records-recs-10\n",
    "dataall_HimagA=np.zeros((num_dataall,len_sequence, k+nf)).astype(complex)\n",
    "dataall_HimagE=np.zeros((num_dataall,len_sequence, k+nf)).astype(complex)\n",
    "for i in range(recs,locations*records-10):\n",
    "    dataall_HimagA[i-recs,:,:]=creatHimag_Alice(i,k=k,nf=nf)\n",
    "    dataall_HimagE[i-recs,:,:]=creatHimag_Eve(i,k=k,nf=nf,d=d)\n",
    "\n",
    "dataall_TW=np.concatenate((dataall_HimagA,dataall_HimagE),axis=0)\n",
    "location=[i for i in range(2)]\n",
    "test_data=dataall_TW\n",
    "test_lab=[val for val in location for i in range(int(num_dataall))]\n",
    "\n",
    "del dataall_HimagA, dataall_HimagE, dataall_TW\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class hDataset(Dataset):\n",
    "    def __init__(self, k, X, y=None):\n",
    "        super(hDataset).__init__()\n",
    "        # self.data = torch.DoubleTensor(abs(X))\n",
    "        self.data = X\n",
    "        self.k = k\n",
    "        if y is not None:\n",
    "            self.label = torch.LongTensor(y)\n",
    "        else:\n",
    "            self.label = None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        h = self.data[idx]\n",
    "        h = convert_pic(h)\n",
    "        if self.label is not None:\n",
    "            return h, self.label[idx]\n",
    "        else:\n",
    "            return h\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import resnet34\n",
    "\n",
    "device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'DEVICE: {device}')\n",
    "\n",
    "import numpy as np\n",
    "test_set = hDataset(k, test_data, test_lab)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "_exp_name=\"Corrm9947_run4_resnettwi_sample\"\n",
    "model_best = resnet34(num_classes=2).to(device)\n",
    "model_best.load_state_dict(torch.load(f\"{_exp_name}_best.ckpt\",map_location='cpu'))\n",
    "model_best.eval()\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "    for data,_ in test_loader:\n",
    "        test_pred = model_best(data.to(device))\n",
    "        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
    "        prediction += test_label.squeeze().tolist()\n",
    "\n",
    "accuracy=1 - np.count_nonzero(np.array(prediction)-np.array(test_lab))/len(prediction)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aerror=np.count_nonzero(np.array(prediction)-np.array(test_lab))/len(prediction)\n",
    "print(aerror)\n",
    "accuracy=1 - aerror\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error=np.array(prediction)-np.array(test_lab)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "plt.plot(error, linestyle=\"--\")\n",
    "# plt.legend(loc=\"best\")\n",
    "plt.xlabel('Records')\n",
    "plt.ylabel('Error')\n",
    "plt.show\n",
    "\n",
    "# import csv\n",
    "# with open('error.txt','w') as cfile:\n",
    "#     w = csv.writer(cfile)\n",
    "#     w.writerow(error)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10a56fe35cef5e20a6d2ddfc0a4e0d073b59b974d8a919d683845127505c73a2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pytorchgpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
